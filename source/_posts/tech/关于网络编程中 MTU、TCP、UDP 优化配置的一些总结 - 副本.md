---
title: 关于网络编程中 MTU、TCP、UDP 优化配置的一些总结
date: 2016-09-13 15:47:21
categories: technology
tags:
---
<div id="cnblogs_post_body"><p><span><span>首先要看 TCP/IP 协议，涉及到四层：链路层，网络层，传输层，应用层。</span>&nbsp;　　<br>其中以太网（Ethernet）的数据帧在链路层 　　<br><strong>IP 包</strong>在网络层 　　<br><strong>TCP 或 UDP 包</strong>在传输层 　　<br><strong>TCP 或 UDP 中的数据</strong>（Data) 在应用层 　　<br>它们的<strong>关系是</strong>&nbsp;数据帧｛IP 包｛TCP 或 UDP 包｛Data｝｝｝ 　　<br>---------------------------------------------------------------------------------<br>在应用程序中我们用到的 Data 的长度最大是多少，直接取决于底层的限制。 　　<br>我们从下到上分析一下： 　　<br>1.<strong> 在链路层</strong>，由以太网的物理特性决定了数据帧的长度为（46＋18）－（1500＋18），其中的 18 是数据帧的头和尾，也就是说<strong>数据帧的内容最大为 1500</strong>（不包括帧头和帧尾），即 MTU（Maximum Transmission Unit）为 1500； 　<br>2.<strong> 在网络层</strong>，因为 IP 包的首部要占用 20 字节，所以这的 MTU 为 1500－20＝1480；　<br>3.<strong> 在传输层</strong>，对于 UDP 包的首部要占用 8 字节，所以这的 MTU 为 1480－8＝1472； 　　<br>所以，在应用层，你的 Data 最大长度为 1472。 （当我们的 UDP 包中的数据多于 MTU(1472) 时，发送方的 IP 层需要分片 fragmentation 进行传输，而在接收方 IP 层则需要进行数据报重组，由于 UDP 是不可靠的传输协议，如果分片丢失导致重组失败，将导致 UDP 数据包被丢弃）。 　　<br>从上面的分析来看，在普通的局域网环境下，UDP 的数据最大为 1472 字节最好（避免分片重组）。 　　<br>但在网络编程中，Internet 中的路由器可能有设置成不同的值（小于默认值），<strong>Internet 上的标准 MTU 值为 576</strong>，所以 Internet 的 UDP 编程时数据长度最好在 576－20－8＝548 字节以内。<br>---------------------------------------------------------------------------------　　<br>MTU 对我们的 UDP 编程很重要，那如何查看路由的 MTU 值呢？ 　　<br>对于 windows OS: ping -f -l 　　如：ping -f -l 1472 192.168.0.1 　　<br>如果提示：Packets needs to be fragmented but DF set. 　　则表明 MTU 小于 1500，不断改小 data_length 值，可以最终测算出 gateway 的 MTU 值； 　　<br>对于 linux OS: ping -c -M do -s 　　如： ping -c 1 -M do -s 1472 192.168.0.1 　　<br>如果提示 Frag needed and DF set…… 　　则表明 MTU 小于 1500，可以再测以推算 gateway 的 MTU。</span></p>
<p><span>&nbsp;</span></p>
<div><span>原理：ping 程序使用 ICMP 报文，ICMP 报文首部占 8 字节，IP 数据报首部占 20 字节，因此在数据大小基础上加上 28 字节为 MTU 值。</span></div>
<p><span>---------------------------------------------------------------------------------　</span></p>
<p><span><strong>IP 数据包的最大长度是 64K 字节 (65535)，</strong>因为在 IP 包头中用 2 个字节描述报文长度，2 个字节所能表达的最大数字就是 65535。&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>由于 IP 协议提供为上层协议<span>分割和重组报文的功能，因此传输层协议的数据包长度原则上来说没有限制。</span>实际上限制还是有的，因为 IP 包的标识字段终究不可能无限长，按照 IPv4，好像上限应该是 4G(64K*64K)。依靠这种机制，<strong>TCP 包头中就没有 “包长度” 字段，而完全依靠 IP 层去处理分帧。这就是为什么 TCP 常常被称作一种 “流协议” 的原因</strong>，开发者在使用 TCP 服务的时候，不必去关心数据包的大小，只需讲 SOCKET 看作一条数据流的入口，往里面放数据就是了，TCP 协议本身会进行拥塞 / 流量控制。&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>UDP 则与 TCP 不同，UDP 包头内有总长度字段，同样为两个字节，因此 UDP 数据包的总长度被限制为 65535，这样恰好可以放进一个 IP 包内，使得 UDP/IP 协议栈的实现非常简单和高效。65535 再减去 UDP 头本身所占据的 8 个字节，UDP 服务中的最大有效载荷长度仅为 65527。这个值也就是你在调用 getsockopt() 时指定 SO_MAX_MSG_SIZE 所得到返回值，任何使用 SOCK_DGRAM 属性的 socket，一次 send 的数据都不能超过这个值，否则必然得到一个错误。&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>那么，IP 包提交给下层协议时将会得到怎样的处理呢？这就取决于数据链路层协议了，一般的数据链路层协议都会负责将 IP 包分割成更小的帧，然后在目的端重组它。在 EtherNet 上，数据链路帧的大小如以上几位大侠所言。而如果是 IP&nbsp;&nbsp; over&nbsp;&nbsp; ATM，则 IP 包将被切分成一个一个的 ATM&nbsp;&nbsp; Cell，大小为 53 字节。</span></p>
<p><span>&nbsp;</span></p>
<p><span>一些典型的 MTU 值：&nbsp;</span></p>
<p><span>网络： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MTU 字节</span><br><span>超通道&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 　　 &nbsp;65535</span><br><span>16Mb/s 信息令牌环（IBM） &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 　　　　17914</span><br><span>4Mb/s 令牌环（IEEE802.5） &nbsp; &nbsp; &nbsp; &nbsp; 　　　　&nbsp;4464</span><br><span>FDDI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;　　　 &nbsp;4352</span><br><span>以太网&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 　　 &nbsp;1500</span><br><span>IEEE802.3/802.2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 　　&nbsp;&nbsp;1492</span><br><span>X.25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;　　 &nbsp; 576</span><br><span>点对点（低时延） &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;　&nbsp; 296</span></p>
<p><span>&nbsp; &nbsp; 路径 MTU：如果两台主机之间的通信要通过多个网络，那么每个网络的链路层就可能有不同的 MTU。重要的不是两台主机所在网络的 MTU 的值，重要的是两台通信主机路径中的最小 MTU。它被称作路径 MTU。</span></p>
<p><strong><span>Tcp 传输中的 nagle 算法</span></strong></p>
<p><span>　　TCP/IP 协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送 ACK 表示确认。为了尽可能的利用网络带宽，TCP 总是希望尽可能的发送足够大的数据。（一个连接会设置 MSS 参数，因此，TCP/IP 希望每次都能够以 MSS 尺寸的数据块来发送数据）。Nagle 算法就是为了尽可能发送大块数据，避免网络中充斥着许多小数据块。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nagle 算法的基本定义是任意时刻，最多只能有一个未被确认的小段。&nbsp;所谓 “小段”，指的是小于 MSS 尺寸的数据块，所谓 “未被确认”，是指一个数据块发送出去后，没有收到对方发送的 ACK 确认该数据已收到。</span></p>
<p><span>1. Nagle 算法的规则：</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（1）如果包长度达到 MSS，则允许发送；</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（2）如果该包含有 FIN，则允许发送；</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（3）设置了 TCP_NODELAY 选项，则允许发送；</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（4）未设置 TCP_CORK 选项时，若所有发出去的小数据包（包长度小于 MSS）均被确认，则允许发送；</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（5）上述条件都未满足，但发生了超时（一般为 200ms），则立即发送。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nagle 算法只允许一个未被 ACK 的包存在于网络，它并不管包的大小，因此它事实上就是一个扩展的停 - 等协议，只不过它是基于包停 - 等的，而不是基于字节停 - 等的。Nagle 算法完全由 TCP 协议的 ACK 机制决定，这会带来一些问题，比如如果对端 ACK 回复很快的话，Nagle 事实上不会拼接太多的数据包，虽然避免了网络拥塞，网络总体的利用率依然很低。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span>Nagle</span><span> 算法是</span><span> silly&nbsp;window&nbsp;syndrome(SWS)</span><span> 预防算法的一个半集。</span>SWS 算法预防发送少量的数据，Nagle 算法是其在发送方的实现，而接收方要做的时不要通告缓冲空间的很小增长，不通知小窗口，除非缓冲区空间有显著的增长。这里显著的增长定义为完全大小的段（MSS）或增长到大于最大窗口的一半。</span></p>
<p><span>&nbsp;注意：BSD 的实现是允许在空闲链接上发送大的写操作剩下的最后的小段，也就是说，当超过 1 个 MSS 数据发送时，内核先依次发送完 n 个 MSS 的数据包，然后再发送尾部的小数据包，其间不再延时等待。（假设网络不阻塞且接收窗口足够大）。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举个例子，一开始 client 端调用 socket 的 write 操作将一个 int 型数据（称为 A 块）写入到网络中，由于此时连接是空闲的（也就是说还没有未被确认的小段），因此这个 int 型数据会被马上发送到 server 端，接着，client 端又调用 write 操作写入‘\r\n’（简称 B 块），这个时候，A 块的 ACK 没有返回，所以可以认为已经存在了一个未被确认的小段，所以 B 块没有立即被发送，一直等待 A 块的 ACK 收到（大概 40ms 之后），B 块才被发送。整个过程如图所示：</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里还隐藏了一个问题，就是 A 块数据的 ACK 为什么 40ms 之后才收到？这是因为 TCP/IP 中不仅仅有 nagle 算法，还有一个 TCP 确认延迟机制&nbsp;。当 Server 端收到数据之后，它并不会马上向 client 端发送 ACK，而是会将 ACK 的发送延迟一段时间（假设为 t），它希望在 t 时间内 server 端会向 client 端发送应答数据，这样 ACK 就能够和应答数据一起发送，就像是应答数据捎带着 ACK 过去。在我之前的时间中，t 大概就是 40ms。这就解释了为什么'\r\n'（B 块）总是在 A 块之后 40ms 才发出。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然，TCP 确认延迟 40ms 并不是一直不变的，TCP 连接的延迟确认时间一般初始化为最小值 40ms，随后根据连接的重传超时时间（RTO）、上次收到数据包与本次接收数据包的时间间隔等参数进行不断调整。另外可以通过设置 TCP_QUICKACK 选项来取消确认延迟。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于 TCP 确认延迟的详细介绍可参考：http://blog.csdn.net/turkeyzhou/article/details/6764389</span></p>
<p><span>2.&nbsp;TCP_NODELAY&nbsp;选项</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，发送数据采用 Negale&nbsp;算法。这样虽然提高了网络吞吐量，但是实时性却降低了，在一些交互性很强的应用程序来说是不允许的，使用 TCP_NODELAY 选项可以禁止 Negale&nbsp;算法。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时，应用程序向内核递交的每个数据包都会立即发送出去。需要注意的是，虽然禁止了 Negale&nbsp;算法，但网络的传输仍然受到 TCP 确认延迟机制的影响。</span></p>
<p><span>3.&nbsp;TCP_CORK&nbsp;选项</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所谓的 CORK 就是塞子的意思，形象地理解就是用 CORK 将连接塞住，使得数据先不发出去，等到拔去塞子后再发出去。设置该选项后，内核会尽力把小数据包拼接成一个大的数据包（一个 MTU）再发送出去，当然若一定时间后（一般为 200ms，该值尚待确认），内核仍然没有组合成一个 MTU 时也必须发送现有的数据（不可能让数据一直等待吧）。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，TCP_CORK 的实现可能并不像你想象的那么完美，CORK 并不会将连接完全塞住。内核其实并不知道应用层到底什么时候会发送第二批数据用于和第一批数据拼接以达到 MTU 的大小，因此内核会给出一个时间限制，在该时间内没有拼接成一个大包（努力接近 MTU）的话，内核就会无条件发送。也就是说若应用层程序发送小包数据的间隔不够短时，TCP_CORK 就没有一点作用，反而失去了数据的实时性（每个小包数据都会延时一定时间再发送）。</span></p>
<p><span>4.&nbsp;Nagle 算法与 CORK 算法区别</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nagle 算法和 CORK 算法非常类似，但是它们的着眼点不一样，Nagle 算法主要避免网络因为太多的小包（协议头的比例非常之大）而拥塞，而 CORK 算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小。如此看来这二者在避免发送小包上是一致的，在用户控制的层面上，Nagle 算法完全不受用户 socket 的控制，你只能简单的设置 TCP_NODELAY 而禁用它，CORK 算法同样也是通过设置或者清除 TCP_CORK 使能或者禁用之，然而 Nagle 算法关心的是网络拥塞问题，只要所有的 ACK 回来则发包，而 CORK 算法却可以关心内容，在前后数据包发送间隔很短的前提下（很重要，否则内核会帮你将分散的包发出），即使你是分散发送多个小数据包，你也可以通过使能 CORK 算法将这些内容拼接在一个包内，如果此时用 Nagle 算法的话，则可能做不到这一点。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;实际上 Nagle 算法并不是很复杂，他的主要职责是数据的累积，实际上有两个门槛：一个就是缓&nbsp;冲区中的字节数达到了一定量，另一个就是等待了一定的时间（一般的 Nagle 算法都是等待 200ms）；这两个门槛的任何一个达到都必须发送数据了。一般&nbsp;情况下，如果数据流量很大，第二个条件是永远不会起作用的，但当发送小的数据包时，第二个门槛就发挥作用了，防止数据被无限的缓存在缓冲区不是好事情哦。&nbsp;了解了 TCP 的 Nagle 算法的原理之后我们可以自己动手来实现一个类似的算法了，在动手之前我们还要记住一个重要的事情，也是我们动手实现 Nagle 算&nbsp;法的主要动机就是我想要紧急发送数据的时候就要发送了，所以对于上面的两个门槛之外还的增加一个门槛就是紧急数据发送。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;对于我现在每秒钟 10 次数据发送，每次数据发送量固定在 85~100 字节的应用而言，如果采用默认的开启 Nagle 算法，我在发送端，固定每帧数据 85 个，间隔 100ms 发送一次，我在接受端（阻塞方式使用）接受的数据是 43&nbsp;138 交替出现，可能就是这个算法的时间阈值问题，如果关闭 Nagle 算法，在接收端就可以保证数据每次接收到的都是 85 帧。</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;Nagle 算法适用于小包、高延迟的场合，而对于要求交互速度的 b/s 或 c/s 就<span>不合适了。</span><span>socket</span><span> 在创建的时候，默认都是使用</span><span> Nagle</span><span> 算法的，这会导致交互速度严重</span>下降，所以需要 setsockopt 函数来设置 TCP_NODELAY 为 1. 不过取消了 Nagle 算法，就会导致 TCP 碎片增多，效率可能会降低。</span></p>
<p><span>关闭 nagle 算法, 以免影响性能，因为控制时控制端要发送很多数据量很小的数据包, 需要马上发送。</span></p>
<p><span>&nbsp;const&nbsp;char&nbsp;chOpt&nbsp;=&nbsp;1;</span></p>
<p><span>int&nbsp;nErr&nbsp;=&nbsp;setsockopt(pContext-&gt;m_Socket,&nbsp;IPPROTO_TCP,&nbsp;TCP_NODELAY,&nbsp;&amp;chOpt,&nbsp;sizeof(char));</span></p>
<p><span>if&nbsp;(nErr&nbsp;==&nbsp;-1)</span></p>
<p><span>{</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;TRACE(_T("setsockopt()&nbsp;error\n"),WSAGetLastError());</span></p>
<p><span>&nbsp;&nbsp;&nbsp;&nbsp;return;</span></p>
<p><span>}</span></p>
<div>
<p><span>setsockopt(sockfd, SOL_TCP, TCP_CORK, &amp;on, sizeof(on)); //set TCP_CORK</span></p>
<p>&nbsp;</p>

</div>
<p><strong><span>TCP 传输小数据包效率问题</span></strong></p>
<p><span>摘要：当使用 TCP 传输小型数据包时，程序的设计是相当重要的。如果在设计方案中不对 TCP 数据包的</span><br><span>延迟应答，Nagle 算法，Winsock 缓冲作用引起重视，将会严重影响程序的性能。这篇文章讨论了这些</span><br><span>问题，列举了两个案例，给出了一些传输小数据包的优化设计方案。</span></p>
<p><span>背景：当 Microsoft TCP 栈接收到一个数据包时，会启动一个 200 毫秒的计时器。当 ACK 确认数据包</span><br><span>发出之后，计时器会复位，接收到下一个数据包时，会再次启动 200 毫秒的计时器。为了提升应用程序</span><br><span>在内部网和 Internet 上的传输性能，Microsoft TCP 栈使用了下面的策略来决定在接收到数据包后</span><br><span>什么时候发送 ACK 确认数据包：</span><br><span>1、如果在 200 毫秒的计时器超时之前，接收到下一个数据包，则立即发送 ACK 确认数据包。</span><br><span>2、如果当前恰好有数据包需要发给 ACK 确认信息的接收端，则把 ACK 确认信息附带在数据包上立即发送。</span><br><span>3、当计时器超时，ACK 确认信息立即发送。</span><br><span>为了避免小数据包拥塞网络，Microsoft TCP 栈默认启用了 Nagle 算法，这个算法能够将应用程序多次</span><br><span>调用 Send 发送的数据拼接起来，当收到前一个数据包的 ACK 确认信息时，一起发送出去。下面是 Nagle</span><br><span>算法的例外情况：</span><br><span>1、如果 Microsoft TCP 栈拼接起来的数据包超过了 MTU 值，这个数据会立即发送，而不等待前一个数据</span><br><span>包的 ACK 确认信息。在以太网中，TCP 的 MTU(Maximum Transmission Unit) 值是 1460 字节。</span><br><span>2、如果设置了 TCP_NODELAY 选项，就会禁用 Nagle 算法，应用程序调用 Send 发送的数据包会立即被</span><br><span>投递到网络，而没有延迟。</span><br><span>为了在应用层优化性能，Winsock 把应用程序调用 Send 发送的数据从应用程序的缓冲区复制到 Winsock</span><br><span>内核缓冲区。Microsoft TCP 栈利用类似 Nagle 算法的方法，决定什么时候才实际地把数据投递到网络。</span><br><span>内核缓冲区的默认大小是 8K，使用 SO_SNDBUF 选项，可以改变 Winsock 内核缓冲区的大小。如果有必要的话，</span><br><span>Winsock 能缓冲大于 SO_SNDBUF 缓冲区大小的数据。在绝大多数情况下，应用程序完成 Send 调用仅仅表明数据</span><br><span>被复制到了 Winsock 内核缓冲区，并不能说明数据就实际地被投递到了网络上。唯一一种例外的情况是：</span><br><span>通过设置 SO_SNDBUT 为 0 禁用了 Winsock 内核缓冲区。</span></p>
<p><span>Winsock 使用下面的规则来向应用程序表明一个 Send 调用的完成：</span><br><span>1、如果 socket 仍然在 SO_SNDBUF 限额内，Winsock 复制应用程序要发送的数据到内核缓冲区，完成 Send 调用。</span><br><span>2、如果 Socket 超过了 SO_SNDBUF 限额并且先前只有一个被缓冲的发送数据在内核缓冲区，Winsock 复制要发送</span><br><span>的数据到内核缓冲区，完成 Send 调用。</span><br><span>3、如果 Socket 超过了 SO_SNDBUF 限额并且内核缓冲区有不只一个被缓冲的发送数据，Winsock 复制要发送的数据</span><br><span>到内核缓冲区，然后投递数据到网络，直到 Socket 降到 SO_SNDBUF 限额内或者只剩余一个要发送的数据，才</span><br><span>完成 Send 调用。</span></p>
<p><span>案例 1</span><br><span>一个 Winsock TCP 客户端需要发送 10000 个记录到 Winsock TCP 服务端，保存到数据库。记录大小从 20 字节到 100</span><br><span>字节不等。对于简单的应用程序逻辑，可能的设计方案如下：</span><br><span>1、客户端以阻塞方式发送，服务端以阻塞方式接收。</span><br><span>2、客户端设置 SO_SNDBUF 为 0，禁用 Nagle 算法，让每个数据包单独的发送。</span><br><span>3、服务端在一个循环中调用 Recv 接收数据包。给 Recv 传递 200 字节的缓冲区以便让每个记录在一次 Recv 调用中</span><br><span>被获取到。</span></p>
<p><span>性能：</span><br><span>在测试中发现，客户端每秒只能发送 5 条数据到服务段，总共 10000 条记录，976K 字节左右，用了半个多小时</span><br><span>才全部传到服务器。</span></p>
<p><span>分析：</span><br><span>因为客户端没有设置 TCP_NODELAY 选项，Nagle 算法强制 TCP 栈在发送数据包之前等待前一个数据包的 ACK 确认</span><br><span>信息。然而，客户端设置 SO_SNDBUF 为 0，禁用了内核缓冲区。因此，10000 个 Send 调用只能一个数据包一个数据</span><br><span>包的发送和确认，由于下列原因，每个 ACK 确认信息被延迟 200 毫秒：</span><br><span>1、当服务器获取到一个数据包，启动一个 200 毫秒的计时器。</span><br><span>2、服务端不需要向客户端发送任何数据，所以，ACK 确认信息不能被发回的数据包顺路携带。</span><br><span>3、客户端在没有收到前一个数据包的确认信息前，不能发送数据包。</span><br><span>4、服务端的计时器超时后，ACK 确认信息被发送到客户端。</span></p>
<p><span>如何提高性能：</span><br><span>在这个设计中存在两个问题。第一，存在延时问题。客户端需要能够在 200 毫秒内发送两个数据包到服务端。</span><br><span>因为客户端默认情况下使用 Nagle 算法，应该使用默认的内核缓冲区，不应该设置 SO_SNDBUF 为 0。一旦 TCP</span><br><span>栈拼接起来的数据包超过 MTU 值，这个数据包会立即被发送，不用等待前一个 ACK 确认信息。第二，这个设计</span><br><span>方案对每一个如此小的的数据包都调用一次 Send。发送这么小的数据包是不很有效率的。在这种情况下，应该</span><br><span>把每个记录补充到 100 字节并且每次调用 Send 发送 80 个记录。为了让服务端知道一次总共发送了多少个记录，</span><br><span>客户端可以在记录前面带一个头信息。</span></p>
<p><span>案例二：</span><br><span>一个 Winsock TCP 客户端程序打开两个连接和一个提供股票报价服务的 Winsock TCP 服务端通信。第一个连接</span><br><span>作为命令通道用来传输股票编号到服务端。第二个连接作为数据通道用来接收股票报价。两个连接被建立后，</span><br><span>客户端通过命令通道发送股票编号到服务端，然后在数据通道上等待返回的股票报价信息。客户端在接收到第一</span><br><span>个股票报价信息后发送下一个股票编号请求到服务端。客户端和服务端都没有设置 SO_SNDBUF 和 TCP_NODELAY</span><br><span>选项。</span></p>
<p><span>性能：</span><br><span>测试中发现，客户端每秒只能获取到 5 条报价信息。</span></p>
<p><span>分析：</span></p>
<p><span>这个设计方案一次只允许获取一条股票信息。第一个股票编号信息通过命令通道发送到服务端，立即接收到</span><br><span>服务端通过数据通道返回的股票报价信息。然后，客户端立即发送第二条请求信息，send 调用立即返回，</span><br><span>发送的数据被复制到内核缓冲区。然而，TCP 栈不能立即投递这个数据包到网络，因为没有收到前一个数据包的</span><br><span>ACK 确认信息。200 毫秒后，服务端的计时器超时，第一个请求数据包的 ACK 确认信息被发送回客户端，客户端</span><br><span>的第二个请求包才被投递到网络。第二个请求的报价信息立即从数据通道返回到客户端，因为此时，客户端的</span><br><span>计时器已经超时，第一个报价信息的 ACK 确认信息已经被发送到服务端。这个过程循环发生。</span></p>
<p><span>如何提高性能：</span><br><span>在这里，两个连接的设计是没有必要的。如果使用一个连接来请求和接收报价信息，股票请求的 ACK 确认信息会</span><br><span>被返回的报价信息立即顺路携带回来。要进一步的提高性能，客户端应该一次调用 Send 发送多个股票请求，服务端</span><br><span>一次返回多个报价信息。如果由于某些特殊原因必须要使用两个单向的连接，客户端和服务端都应该设置 TCP_NODELAY</span><br><span>选项，让小数据包立即发送而不用等待前一个数据包的 ACK 确认信息。</span></p>
<p><span>提高性能的建议：</span><br><span>上面两个案例说明了一些最坏的情况。当设计一个方案解决大量的小数据包发送和接收时，应该遵循以下的建议：</span><br><span>1、如果数据片段不需要紧急传输的话，应用程序应该将他们拼接成更大的数据块，再调用 Send。因为发送缓冲区</span><br><span>很可能被复制到内核缓冲区，所以缓冲区不应该太大，通常比 8K 小一点点是很有效率的。只要 Winsock 内核缓冲区</span><br><span>得到一个大于 MTU 值的数据块，就会发送若干个数据包，剩下最后一个数据包。发送方除了最后一个数据包，都不会</span><br><span>被 200 毫秒的计时器触发。</span><br><span>2、如果可能的话，避免单向的 Socket 数据流接连。</span><br><span>3、不要设置 SO_SNDBUF 为 0，除非想确保数据包在调用 Send 完成之后立即被投递到网络。事实上，8K 的缓冲区适合大多数</span><br><span>情况，不需要重新改变，除非新设置的缓冲区经过测试的确比默认大小更高效。</span><br><span>4、如果数据传输不用保证可靠性，使用 UDP。</span></p></div>